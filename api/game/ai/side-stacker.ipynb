{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 115200,
     "status": "ok",
     "timestamp": 1746508342247,
     "user": {
      "displayName": "Alex Lee",
      "userId": "12899629336782282444"
     },
     "user_tz": 180
    },
    "id": "TAa2lLdA-rEI",
    "outputId": "ce8035c8-0d9f-4de9-ceae-5afcb88ce546"
   },
   "outputs": [],
   "source": [
    "# ✅ 1. Install PyTorch (if needed)\n",
    "!pip install torch\n",
    "\n",
    "# ✅ 2. Import dependencies\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "\n",
    "# ✅ 3. Define constants\n",
    "BOARD_SIZE = 7\n",
    "GAMES_PER_ITERATION = 5  # reduce for testing, increase later\n",
    "REPLAY_BUFFER_SIZE = 50000\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 2\n",
    "NUM_ITERATIONS = 10000  # set high (e.g. 10000) for full training\n",
    "MODEL_PATH = \"side_stacker_model.pth\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1746508353550,
     "user": {
      "displayName": "Alex Lee",
      "userId": "12899629336782282444"
     },
     "user_tz": 180
    },
    "id": "8R1kiUD1-snW"
   },
   "outputs": [],
   "source": [
    "# ✅ 4. Model definition\n",
    "class SideStackerNet(nn.Module):\n",
    "    def __init__(self, board_size=7):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(2, 64, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.policy_head = nn.Linear(64 * board_size * board_size, board_size * 2)\n",
    "        self.value_head = nn.Linear(64 * board_size * board_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        policy = self.policy_head(x)\n",
    "        value = torch.tanh(self.value_head(x))\n",
    "        return policy, value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1746508357856,
     "user": {
      "displayName": "Alex Lee",
      "userId": "12899629336782282444"
     },
     "user_tz": 180
    },
    "id": "TEEND_Km-uJ4"
   },
   "outputs": [],
   "source": [
    "# ✅ 5. Game utils\n",
    "def board_to_tensor(board, player):\n",
    "    p1 = [[1 if cell == player else 0 for cell in row] for row in board]\n",
    "    p2 = [[1 if cell == -player else 0 for cell in row] for row in board]\n",
    "    tensor = torch.tensor([p1, p2], dtype=torch.float32).unsqueeze(0)\n",
    "    return tensor.to(device)\n",
    "\n",
    "def get_valid_moves(board):\n",
    "    valid_moves = []\n",
    "    for row_index, row in enumerate(board):\n",
    "        try:\n",
    "            left_index = row.index(0)\n",
    "            valid_moves.append((row_index, 'L'))\n",
    "        except ValueError:\n",
    "            pass\n",
    "        try:\n",
    "            right_index = len(row) - 1 - row[::-1].index(0)\n",
    "            if right_index != left_index:\n",
    "                valid_moves.append((row_index, 'R'))\n",
    "        except ValueError:\n",
    "            pass\n",
    "    return valid_moves\n",
    "\n",
    "def apply_move(board, row, direction, player):\n",
    "    board = [list(r) for r in board]\n",
    "    if direction == 'L':\n",
    "        for col in range(BOARD_SIZE):\n",
    "            if board[row][col] == 0:\n",
    "                board[row][col] = player\n",
    "                break\n",
    "    else:\n",
    "        for col in reversed(range(BOARD_SIZE)):\n",
    "            if board[row][col] == 0:\n",
    "                board[row][col] = player\n",
    "                break\n",
    "    return board\n",
    "\n",
    "def check_winner(board):\n",
    "    def check_line(line):\n",
    "        for i in range(len(line) - 3):\n",
    "            window = line[i:i+4]\n",
    "            if sum(window) == 4:\n",
    "                return 1\n",
    "            elif sum(window) == -4:\n",
    "                return -1\n",
    "        return 0\n",
    "\n",
    "    for row in board:\n",
    "        if (res := check_line(row)) != 0:\n",
    "            return res\n",
    "    for col in zip(*board):\n",
    "        if (res := check_line(col)) != 0:\n",
    "            return res\n",
    "    for d in range(-BOARD_SIZE + 1, BOARD_SIZE):\n",
    "        diag1 = [board[i][i - d] for i in range(max(d, 0), min(BOARD_SIZE + d, BOARD_SIZE)) if 0 <= i - d < BOARD_SIZE]\n",
    "        diag2 = [board[i][BOARD_SIZE - 1 - i + d] for i in range(max(-d, 0), min(BOARD_SIZE - d, BOARD_SIZE)) if 0 <= BOARD_SIZE - 1 - i + d < BOARD_SIZE]\n",
    "        if (res := check_line(diag1)) != 0:\n",
    "            return res\n",
    "        if (res := check_line(diag2)) != 0:\n",
    "            return res\n",
    "    return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1746508722961,
     "user": {
      "displayName": "Alex Lee",
      "userId": "12899629336782282444"
     },
     "user_tz": 180
    },
    "id": "gJYBipCU-v1u"
   },
   "outputs": [],
   "source": [
    "# ✅ 6. Self-play training logic\n",
    "def self_play_game(model):\n",
    "    board = [[0] * BOARD_SIZE for _ in range(BOARD_SIZE)]\n",
    "    player = 1\n",
    "    history = []\n",
    "\n",
    "    while True:\n",
    "        state_tensor = board_to_tensor(board, player)\n",
    "        with torch.no_grad():\n",
    "            logits, _ = model(state_tensor)\n",
    "            probs = torch.softmax(logits, dim=1).cpu().numpy()[0]\n",
    "\n",
    "        move_map = [(row, d) for row in range(BOARD_SIZE) for d in ['L', 'R']]\n",
    "        valid_moves = get_valid_moves(board)\n",
    "        legal = [(i, move_map[i]) for i in range(len(move_map)) if move_map[i] in valid_moves]\n",
    "        if not legal:\n",
    "            break\n",
    "\n",
    "        legal_indices = [i for i, _ in legal]\n",
    "        legal_probs = [probs[i] for i in legal_indices]\n",
    "        prob_array = np.array(legal_probs, dtype=np.float64)\n",
    "\n",
    "        # Handle total probability safely\n",
    "        total = prob_array.sum()\n",
    "        if total == 0 or np.isnan(total):\n",
    "            # fallback: uniform distribution over legal moves\n",
    "            prob_array = np.ones_like(prob_array) / len(prob_array)\n",
    "        else:\n",
    "            prob_array = prob_array / total\n",
    "\n",
    "        # Final precision correction\n",
    "        prob_array = np.clip(prob_array, 0, 1)\n",
    "        prob_array = prob_array / prob_array.sum()  # ensure exact normalization\n",
    "\n",
    "        # Optional debug assertion\n",
    "        if not np.isclose(prob_array.sum(), 1.0):\n",
    "            # fallback: uniform\n",
    "            prob_array = np.ones_like(prob_array) / len(prob_array)\n",
    "\n",
    "\n",
    "        chosen = np.random.choice(len(prob_array), p=prob_array)\n",
    "        move = move_map[legal_indices[chosen]]\n",
    "\n",
    "        policy_tensor = torch.zeros(len(move_map), dtype=torch.float32)\n",
    "        policy_tensor[legal_indices[chosen]] = 1.0\n",
    "\n",
    "        history.append((board_to_tensor(board, player).squeeze(0), policy_tensor, player))\n",
    "        board = apply_move(board, move[0], move[1], player)\n",
    "        winner = check_winner(board)\n",
    "        if winner != 0:\n",
    "            break\n",
    "        player *= -1\n",
    "\n",
    "    if winner != 0:\n",
    "        return [(s, p, torch.tensor([1.0 if pl == winner else -1.0])) for s, p, pl in history]\n",
    "    else:\n",
    "        return [(s, p, torch.tensor([0.0])) for s, p, pl in history]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1746508726941,
     "user": {
      "displayName": "Alex Lee",
      "userId": "12899629336782282444"
     },
     "user_tz": 180
    },
    "id": "_VtB_sxU-x5P"
   },
   "outputs": [],
   "source": [
    "# ✅ 7. Train loop\n",
    "def train_model(model, replay_buffer):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    model.train()\n",
    "    for epoch in range(EPOCHS):\n",
    "        random.shuffle(replay_buffer)\n",
    "        for i in range(0, len(replay_buffer), BATCH_SIZE):\n",
    "            batch = replay_buffer[i:i + BATCH_SIZE]\n",
    "            if len(batch) < BATCH_SIZE:\n",
    "                continue\n",
    "            states, policies, values = zip(*batch)\n",
    "            states = torch.stack(states).to(device)\n",
    "            policies = torch.stack(policies).to(device)\n",
    "            values = torch.stack(values).to(device)\n",
    "\n",
    "            pred_policies, pred_values = model(states)\n",
    "            loss = F.cross_entropy(pred_policies, policies) + F.mse_loss(pred_values.squeeze(), values.squeeze())\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EYT9B2l4-zVB",
    "outputId": "0242a73c-5f4a-48b7-b1f4-406f000ad59d"
   },
   "outputs": [],
   "source": [
    "# ✅ 8. Main run function\n",
    "def run_training():\n",
    "    model = SideStackerNet().to(device)\n",
    "    replay_buffer = deque(maxlen=REPLAY_BUFFER_SIZE)\n",
    "\n",
    "    for iteration in range(NUM_ITERATIONS):\n",
    "        print(f\"--- Iteration {iteration + 1}/{NUM_ITERATIONS} ---\")\n",
    "        for _ in range(GAMES_PER_ITERATION):\n",
    "            game_data = self_play_game(model)\n",
    "            replay_buffer.extend(game_data)\n",
    "\n",
    "        print(f\"Training on {len(replay_buffer)} samples...\")\n",
    "        train_model(model, list(replay_buffer))\n",
    "\n",
    "        # Save after each iteration for safety\n",
    "        torch.save(model.state_dict(), MODEL_PATH)\n",
    "\n",
    "    print(\"✅ Training complete.\")\n",
    "\n",
    "run_training()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "executionInfo": {
     "elapsed": 31,
     "status": "error",
     "timestamp": 1746524203424,
     "user": {
      "displayName": "Alex Lee",
      "userId": "12899629336782282444"
     },
     "user_tz": 180
    },
    "id": "gIwf6F2o-1PI",
    "outputId": "98dc9ffd-2852-4b6a-fece-4930b742fa6a"
   },
   "outputs": [],
   "source": [
    "# ✅ 9. Download the trained model\n",
    "from google.colab import files\n",
    "files.download(\"side_stacker_model.pth\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyP/XS3Hu6ZNzVs5Bo5CNDqR",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
