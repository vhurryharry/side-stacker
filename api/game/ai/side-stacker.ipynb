{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TAa2lLdA-rEI"
      },
      "outputs": [],
      "source": [
        "# ✅ 1. Install PyTorch (if needed)\n",
        "!pip install torch\n",
        "\n",
        "# ✅ 2. Import dependencies\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import random\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "from google.colab import files\n",
        "\n",
        "# ✅ 3. Define constants\n",
        "BOARD_SIZE = 7\n",
        "GAMES_PER_ITERATION = 5  # reduce for testing, increase later\n",
        "REPLAY_BUFFER_SIZE = 10000\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 2\n",
        "NUM_ITERATIONS = 3000  # set high (e.g. 10000) for full training\n",
        "MODEL_PATH = \"side_stacker_model.pth\"\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8R1kiUD1-snW"
      },
      "outputs": [],
      "source": [
        "# ✅ 4. Model definition\n",
        "class SideStackerNet(nn.Module):\n",
        "    def __init__(self, board_size=7):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(2, 64, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
        "        self.policy_head = nn.Linear(64 * board_size * board_size, board_size * 2)\n",
        "        self.value_head = nn.Linear(64 * board_size * board_size, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = x.view(x.size(0), -1)\n",
        "        policy = self.policy_head(x)\n",
        "        value = torch.tanh(self.value_head(x))\n",
        "        return policy, value\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TEEND_Km-uJ4"
      },
      "outputs": [],
      "source": [
        "# ✅ 5. Game utils\n",
        "def board_to_tensor(board, player):\n",
        "    p1 = [[1 if cell == player else 0 for cell in row] for row in board]\n",
        "    p2 = [[1 if cell == -player else 0 for cell in row] for row in board]\n",
        "    tensor = torch.tensor([p1, p2], dtype=torch.float32).unsqueeze(0)\n",
        "    return tensor.to(device)\n",
        "\n",
        "def get_valid_moves(board):\n",
        "    valid_moves = []\n",
        "    for row_index, row in enumerate(board):\n",
        "        try:\n",
        "            left_index = row.index(0)\n",
        "            valid_moves.append((row_index, 'L'))\n",
        "        except ValueError:\n",
        "            pass\n",
        "        try:\n",
        "            right_index = len(row) - 1 - row[::-1].index(0)\n",
        "            if right_index != left_index:\n",
        "                valid_moves.append((row_index, 'R'))\n",
        "        except ValueError:\n",
        "            pass\n",
        "    return valid_moves\n",
        "\n",
        "def apply_move(board, row, direction, player):\n",
        "    board = [list(r) for r in board]\n",
        "    if direction == 'L':\n",
        "        for col in range(BOARD_SIZE):\n",
        "            if board[row][col] == 0:\n",
        "                board[row][col] = player\n",
        "                break\n",
        "    else:\n",
        "        for col in reversed(range(BOARD_SIZE)):\n",
        "            if board[row][col] == 0:\n",
        "                board[row][col] = player\n",
        "                break\n",
        "    return board\n",
        "\n",
        "def check_winner(board):\n",
        "    def check_line(line):\n",
        "        for i in range(len(line) - 3):\n",
        "            window = line[i:i+4]\n",
        "            if sum(window) == 4:\n",
        "                return 1\n",
        "            elif sum(window) == -4:\n",
        "                return -1\n",
        "        return 0\n",
        "\n",
        "    for row in board:\n",
        "        if (res := check_line(row)) != 0:\n",
        "            return res\n",
        "    for col in zip(*board):\n",
        "        if (res := check_line(col)) != 0:\n",
        "            return res\n",
        "    for d in range(-BOARD_SIZE + 1, BOARD_SIZE):\n",
        "        diag1 = [board[i][i - d] for i in range(max(d, 0), min(BOARD_SIZE + d, BOARD_SIZE)) if 0 <= i - d < BOARD_SIZE]\n",
        "        diag2 = [board[i][BOARD_SIZE - 1 - i + d] for i in range(max(-d, 0), min(BOARD_SIZE - d, BOARD_SIZE)) if 0 <= BOARD_SIZE - 1 - i + d < BOARD_SIZE]\n",
        "        if (res := check_line(diag1)) != 0:\n",
        "            return res\n",
        "        if (res := check_line(diag2)) != 0:\n",
        "            return res\n",
        "    return 0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gJYBipCU-v1u"
      },
      "outputs": [],
      "source": [
        "# ✅ 6. Self-play training logic\n",
        "def self_play_game(model):\n",
        "    board = [[0] * BOARD_SIZE for _ in range(BOARD_SIZE)]\n",
        "    player = 1\n",
        "    history = []\n",
        "\n",
        "    while True:\n",
        "        state_tensor = board_to_tensor(board, player)\n",
        "        with torch.no_grad():\n",
        "            logits, _ = model(state_tensor)\n",
        "            probs = torch.softmax(logits, dim=1).cpu().numpy()[0]\n",
        "\n",
        "        move_map = [(row, d) for row in range(BOARD_SIZE) for d in ['L', 'R']]\n",
        "        valid_moves = get_valid_moves(board)\n",
        "        legal = [(i, move_map[i]) for i in range(len(move_map)) if move_map[i] in valid_moves]\n",
        "        if not legal:\n",
        "            break\n",
        "\n",
        "        legal_indices = [i for i, _ in legal]\n",
        "        legal_probs = [probs[i] for i in legal_indices]\n",
        "        prob_array = np.array(legal_probs, dtype=np.float64)\n",
        "\n",
        "        # Handle total probability safely\n",
        "        total = prob_array.sum()\n",
        "        if total == 0 or np.isnan(total):\n",
        "            # fallback: uniform distribution over legal moves\n",
        "            prob_array = np.ones_like(prob_array) / len(prob_array)\n",
        "        else:\n",
        "            prob_array = prob_array / total\n",
        "\n",
        "        # Final precision correction\n",
        "        prob_array = np.clip(prob_array, 0, 1)\n",
        "        prob_array = prob_array / prob_array.sum()  # ensure exact normalization\n",
        "\n",
        "        # Optional debug assertion\n",
        "        if not np.isclose(prob_array.sum(), 1.0):\n",
        "            # fallback: uniform\n",
        "            prob_array = np.ones_like(prob_array) / len(prob_array)\n",
        "\n",
        "\n",
        "        chosen = np.random.choice(len(prob_array), p=prob_array)\n",
        "        move = move_map[legal_indices[chosen]]\n",
        "\n",
        "        policy_tensor = torch.zeros(len(move_map), dtype=torch.float32)\n",
        "        policy_tensor[legal_indices[chosen]] = 1.0\n",
        "\n",
        "        history.append((board_to_tensor(board, player).squeeze(0), policy_tensor, player))\n",
        "        board = apply_move(board, move[0], move[1], player)\n",
        "        winner = check_winner(board)\n",
        "        if winner != 0:\n",
        "            break\n",
        "        player *= -1\n",
        "\n",
        "    if winner != 0:\n",
        "        return [(s, p, torch.tensor([1.0 if pl == winner else -1.0])) for s, p, pl in history]\n",
        "    else:\n",
        "        return [(s, p, torch.tensor([0.0])) for s, p, pl in history]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_VtB_sxU-x5P"
      },
      "outputs": [],
      "source": [
        "# ✅ 7. Train loop\n",
        "def train_model(model, replay_buffer):\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "    model.train()\n",
        "    for epoch in range(EPOCHS):\n",
        "        random.shuffle(replay_buffer)\n",
        "        for i in range(0, len(replay_buffer), BATCH_SIZE):\n",
        "            batch = replay_buffer[i:i + BATCH_SIZE]\n",
        "            if len(batch) < BATCH_SIZE:\n",
        "                continue\n",
        "            states, policies, values = zip(*batch)\n",
        "            states = torch.stack(states).to(device)\n",
        "            policies = torch.stack(policies).to(device)\n",
        "            values = torch.stack(values).to(device)\n",
        "\n",
        "            pred_policies, pred_values = model(states)\n",
        "            loss = F.cross_entropy(pred_policies, policies) + F.mse_loss(pred_values.squeeze(), values.squeeze())\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WjYJAgw9Z-Yt",
        "outputId": "70de3ae6-f234-4ef1-c8e8-30bbe600d66a"
      },
      "outputs": [],
      "source": [
        "# ✅ Mount Google Drive first\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# ✅ Load existing model from Drive (if it exists)\n",
        "model_path = \"/content/drive/MyDrive/hard_model.pth\"\n",
        "\n",
        "try:\n",
        "    model = SideStackerNet().to(device)\n",
        "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "    print(\"✅ Loaded model from:\", model_path)\n",
        "except Exception as e:\n",
        "    print(\"⚠️ No existing model found or load failed. Starting fresh.\", e)\n",
        "    model = SideStackerNet().to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EYT9B2l4-zVB",
        "outputId": "a4283013-25dc-400e-be16-b8607fd1bf3d"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "\n",
        "# ✅ 8. Main run function\n",
        "def run_training():\n",
        "    replay_buffer = deque(maxlen=REPLAY_BUFFER_SIZE)\n",
        "\n",
        "    for iteration in range(NUM_ITERATIONS):\n",
        "        print(f\"--- Iteration {iteration + 1}/{NUM_ITERATIONS} ---\")\n",
        "\n",
        "        for _ in range(GAMES_PER_ITERATION):\n",
        "            game_data = self_play_game(model)\n",
        "            replay_buffer.extend(game_data)\n",
        "\n",
        "        print(f\"Training on {len(replay_buffer)} samples...\")\n",
        "        train_model(model, list(replay_buffer))\n",
        "\n",
        "        # Save to Drive\n",
        "        torch.save(model.state_dict(), model_path)\n",
        "        print(f\"Saved model checkpoint to {model_path}\")\n",
        "\n",
        "        # Free memory\n",
        "        del game_data\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "    print(\"✅ Training complete.\")\n",
        "\n",
        "\n",
        "run_training()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
